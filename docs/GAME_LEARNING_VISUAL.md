# Game Learning: Visual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GAME LEARNING PIPELINE OVERVIEW                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1-2: DATA COLLECTION & ENCODING (Months 1-4)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                                    
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Player     â”‚        â”‚  Maelstrom   â”‚        â”‚  AAS Hub     â”‚
   â”‚   (Human)    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  (C# Client) â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Recorder    â”‚
   â”‚   Gameplay   â”‚ Actionsâ”‚  OCR+Vision  â”‚ gRPC   â”‚  Plugin      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                                  â”‚ Screenshots            â”‚ State+Action
                                  â–¼                        â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Vision     â”‚        â”‚   Episode    â”‚
                            â”‚   Encoder    â”‚        â”‚   Dataset    â”‚
                            â”‚  (ViT/ResNet)â”‚        â”‚   (JSON)     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  Unified State   â”‚
                                  â”‚  [Vision(512) +  â”‚
                                  â”‚   Stats(50) +    â”‚
                                  â”‚   Temporal(128)] â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Phase 3: BEHAVIORAL CLONING (Months 4-7)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚              TRAINING LOOP                               â”‚
   â”‚                                                           â”‚
   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
   â”‚   â”‚ Episode  â”‚â”€â”€â”€â–¶â”‚  Policy  â”‚â”€â”€â”€â–¶â”‚  Train   â”‚         â”‚
   â”‚   â”‚ Dataset  â”‚    â”‚  Network â”‚    â”‚  (SGD)   â”‚         â”‚
   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
   â”‚                         â”‚               â”‚                â”‚
   â”‚                         â”‚ Checkpoints   â”‚ Loss          â”‚
   â”‚                         â–¼               â–¼                â”‚
   â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
   â”‚                   â”‚  Model   â”‚    â”‚TensorBoardâ”‚         â”‚
   â”‚                   â”‚ Storage  â”‚    â”‚   Logs    â”‚         â”‚
   â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ Trained Model
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Inference Engine â”‚
                       â”‚  (Real-time)      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ Action Prediction
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Maelstrom      â”‚
                       â”‚   Command API    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ Execute
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Wizard101      â”‚
                       â”‚   (Game)         â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Phase 4: GHOST MODE (Months 7-9)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Human Player â”€â”
                 â”œâ”€â”€â–¶ Decision Point â—„â”€â”€â”€ AI Policy
                 â”‚         â”‚
   Override â”€â”€â”€â”€â”€â”˜         â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Action    â”‚
                    â”‚   Executed  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Corrections â”‚
                    â”‚   Logged    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  DAgger     â”‚
                    â”‚  Retraining â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Phase 5-6: MULTI-TASK & RL (Months 9-18)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        TASK-CONDITIONED POLICY                       â”‚
   â”‚                                                       â”‚
   â”‚   [State] + [Task Embedding]  â”€â”€â–¶  [Action Probs]  â”‚
   â”‚      â”‚            â”‚                        â”‚         â”‚
   â”‚      â”‚            â”‚                        â”‚         â”‚
   â”‚   Vision      "Complete           Move Forward      â”‚
   â”‚   Features    Unicorn Way"        Cast Spell        â”‚
   â”‚                                    ...               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ RL Optimization (PPO/SAC)
                           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      SELF-PLAY ENVIRONMENT                           â”‚
   â”‚                                                       â”‚
   â”‚   Agent â”€â”€â”€â”€â”€â–¶ Environment â”€â”€â”€â”€â”€â–¶ Reward            â”‚
   â”‚     â–²              â”‚                â”‚                â”‚
   â”‚     â”‚              â”‚                â”‚                â”‚
   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
   â”‚         (Experience Replay Buffer)                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


DATA FLOW SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Raw Gameplay â”€â”€â–¶ Screenshots + Actions â”€â”€â–¶ Training Dataset â”€â”€â–¶ Trained Model â”€â”€â–¶ Autonomous Agent


TECH STACK
â•â•â•â•â•â•â•â•â•â•

â”œâ”€ Data Collection
â”‚  â”œâ”€ Python (CaptureManager)
â”‚  â”œâ”€ gRPC (State streaming)
â”‚  â””â”€ OpenCV (Screenshot capture)
â”‚
â”œâ”€ ML Training
â”‚  â”œâ”€ PyTorch (Models)
â”‚  â”œâ”€ Stable-Baselines3 (RL)
â”‚  â”œâ”€ HuggingFace (Vision encoders)
â”‚  â””â”€ TensorBoard (Monitoring)
â”‚
â””â”€ Deployment
   â”œâ”€ AAS Hub (Orchestration)
   â”œâ”€ Maelstrom (Game control)
   â””â”€ Redis (State caching)


FILE STRUCTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•

plugins/imitation_learning/
â”œâ”€â”€ capture_manager.py      â† Phase 1: Data collection
â”œâ”€â”€ vision_encoder.py        â† Phase 2: State encoding
â”œâ”€â”€ models/
â”‚   â””â”€â”€ policy_network.py    â† Phase 3: BC model
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ bc_trainer.py        â† Phase 3: Training loop
â”‚   â””â”€â”€ rl_trainer.py        â† Phase 6: RL training
â”œâ”€â”€ inference_engine.py      â† Phase 3: Real-time inference
â””â”€â”€ ghost_mode.py            â† Phase 4: Human-in-the-loop

data/
â”œâ”€â”€ episodes/                â† Recorded gameplay
â”œâ”€â”€ models/                  â† Trained checkpoints
â””â”€â”€ logs/                    â† TensorBoard logs


KEY METRICS
â•â•â•â•â•â•â•â•â•â•â•

Phase 1-2: Foundation
â”œâ”€ Dataset Size: 20+ hours
â”œâ”€ Vision Accuracy: 95%+
â””â”€ Data Quality: <5% corruption

Phase 3-4: Behavioral Cloning
â”œâ”€ Quest Success: 80% â†’ 90%
â”œâ”€ Action Accuracy: 85%+
â””â”€ Latency: <100ms

Phase 5-6: Advanced
â”œâ”€ Transfer Learning: 3+ new quests
â”œâ”€ RL Efficiency: +20% vs human
â””â”€ Safety: 0 critical errors


TIMELINE
â•â•â•â•â•â•â•â•

Month 1-2   â–ˆâ–ˆâ–ˆâ–“â–“â–“â–“â–“â–“â–“â–“â–“  Phase 1: Data Collection
Month 2-4   â–“â–“â–“â–ˆâ–ˆâ–ˆâ–“â–“â–“â–“â–“â–“  Phase 2: Vision Encoding
Month 4-7   â–“â–“â–“â–“â–“â–“â–ˆâ–ˆâ–ˆâ–“â–“â–“  Phase 3: Behavioral Cloning â­
Month 7-9   â–“â–“â–“â–“â–“â–“â–“â–“â–ˆâ–ˆâ–ˆâ–“  Phase 4: Ghost Mode
Month 9-12  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–ˆâ–ˆâ–ˆ  Phase 5: Multi-Task
Month 12-18 â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–ˆâ–ˆ  Phase 6: Reinforcement Learning

Legend: â–ˆ Active  â–“ Waiting


QUICK START
â•â•â•â•â•â•â•â•â•â•â•

1. Install Dependencies
   $ pip install torch stable-baselines3 tensorboard

2. Setup Data Dirs
   $ python scripts/setup_ml_dirs.py

3. Record First Episode
   $ python scripts/collect_demonstrations.py

4. Train Policy (Phase 3)
   $ python scripts/train_policy.py --epochs 50

5. Deploy Model
   $ python core/main.py  # Auto-loads trained model


RELATED DOCS
â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ GAME_AUTOMATION_ROADMAP.md    - Full 6-phase plan
ğŸ› ï¸ GAME_LEARNING_INTEGRATION.md  - Developer guide
ğŸ“Š GAME_LEARNING_STATUS.md        - Current progress
ğŸ  README.md                      - Project overview
```
